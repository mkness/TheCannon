\documentclass[pdftex]{beamer}
\input{hogg_presentation} % hogg standard colors
%\input{vc}
% 1.77778 is the ratio of 16 to 9
%\setlength{\paperheight}{3.5in}
%\setlength{\paperwidth}{1.77778\paperheight}
% 1.33333 is the ratio of 4 to 3
\setlength{\paperheight}{4.0in}
\setlength{\paperwidth}{1.33333\paperheight}
\setlength{\textwidth}{0.85\paperwidth}

\newcommand{\credits}{{\footnotesize (Ness, Hogg, \etal)}}
\newcommand{\teff}{T_{\mathrm{eff}}}
\newcommand{\logg}{\log g}
\newcommand{\feh}{[\mathrm{Fe / H}]}

\title{Data-driven models of stars}
\author[David W. Hogg (NYU)]{David W. Hogg \\
  \textsl{\small Center for Cosmology and Particle Physics,
                 New York University} \\
  \textsl{\small Center for Data Science,
                 New York University} \\
  \textsl{\small Max-Planck-Insitut f\"ur Astronomie, Heidelberg}}
\date{2015 January 20}

\newcommand{\conclusions}{%
\begin{frame}
  \frametitle{conclusions}
  \begin{itemize}
  \item A data-driven label transfer system provides \apogee\ stars with labels comparable in quality to the \apogee\ physics-driven pipeline.
    \begin{itemize}
    \item \tc\ (Ness \etal, \textit{ApJ} submitted)
    \item ``labels'' $\equiv (\teff,\logg,\feh)$
    \end{itemize}
  \item \tc\ uses \emph{no physical model of stars}.
  \item The method and the training set are both immature zeroth steps.
  \item There is hope for a consistent system of stellar parameters and chemical abundances across all future surveys.
    \begin{itemize}
    \item \gaia\ \& \project{Gaia-ESO}
    \item \sdssiii\ \project{SEGUE} \& \apogee; \sdssiv\ \project{APOGEE-2}
    \item \project{HERMES} / \project{GALAH}
    \end{itemize}
  \item \texttt{\giturl}
  \end{itemize}
\end{frame}}

\begin{document}

\begin{frame}
  \titlepage
  in collaboration with:\\
  \emph{Melissa~Ness}~(MPIA), Hans-Walter~Rix~(MPIA), \\
  Anna~Ho~(MPIA), and Gail~Zasowski~(JHU)
\end{frame}

%\conclusions

\begin{frame}
  \frametitle{Annie Jump Cannon}
  \begin{itemize}
  \item understood the temperature sequence of stars without the benefit of physical models
    \begin{itemize}
    \item data-driven non-linear dimensionality reduction
    \item ``manifold learning''
    \item (using a huge amount of prior knowledge)
    \end{itemize}
  \item namesake of \tc
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{the paradox of precision astrophysics}
  \begin{itemize}
  \item models are incredibly \emph{explanatory}
    \begin{itemize}
    \item $\Lambda$CDM
    \item stellar spectroscopy
    \item helioseismology
    \end{itemize}
  \item and yet...
  \item<2-> models are \emph{wrong} (ruled out) in detail
    \begin{itemize}
    \item $\chi^2 \gg \nu$
    \item ``The $\chi^2$ statistic is a measure of the size of your data!''
    \end{itemize}
  \item<2-> missing physics, approximation, computation, \emph{gastrophysics}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{physics-driven models}
  \begin{itemize}
  \item put in everything you know
    \begin{itemize}
    \item gravity, atomic and molecular transitions, radiation
    \end{itemize}
  \item make approximations to make things computable
    \begin{itemize}
    \item subgrid models, mixing length, etc
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{machine learning}
  \begin{itemize}
  \item the most extreme of data-driven models
  \item ``the data \emph{is} the model''
    \begin{itemize}
    \item none of your knowledge is relevant
    \end{itemize}
  \item learn (fit) an exceedingly flexible model
    \begin{itemize}
    \item explain or cluster the data
    \item transformation from data to ``labels''
    \end{itemize}
  \item concept of non-parametrics
  \item concept of train, validate, and test
  \item many packages and implementations to choose from
    \begin{itemize}
    \item (and outrageous successes)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{when does machine learning help you?}
  \begin{itemize}
  \item train \& test situation
  \item training data are statistically identical to the test data
    \begin{itemize}
    \item same noise amplitude
    \item same distance or redshift distribution
    \item same luminosity distribution
    \item \emph{never true!}
    \end{itemize}
  \item training data have accurate and precise labels
  \item therefore, we \emph{can't use vanilla machine learning!}
    \begin{itemize}
    \item (no astronomers ever can)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{data-driven models (my personal usage)}
  \begin{itemize}
  \item make use of things you \emph{strongly believe}
    \begin{itemize}
    \item noise model \& instrument resolution
    \item causal structure (shared parameters)
    \end{itemize}
  \item capitalize on huge amounts of data
  \item exceedingly flexible model
  \item concept of train, validate, and test
  \item every situation will be \emph{bespoke}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{why stellar parameters and chemical abundances?}
  \begin{itemize}
  \item top priority for many new projects
    \begin{itemize}
    \item \gaia\ \& \project{Gaia-ESO}
    \item \project{HERMES} \& \project{GALAH}
    \item \sdssiii\ \apogee
    \end{itemize}
  \item extended distribution functions
  \item chemical tagging
    \begin{itemize}
    \item the dynamics and formation of the Milky Way
    \end{itemize}
  \item terrifying inconsistencies in current approaches
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{label transfer for stars}
  \begin{itemize}
  \item a few of your stars have good labels (from somewhere)
  \item can you use this to label the other stars?
  \item why would you want to do this?
    \begin{itemize}
    \item<2> you don't have good models at your wavelengths?
    \item<2> you want two surveys to be on the same ``system''?
    \item<2> you have some stars at high SNR, some at low SNR?
    \item<2> you spent human time on some stars but can't on all?
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\sdssiii\ \apogee}
  \begin{itemize}
  \item Galactic archaeology
  \item \apogee\ DR10: 56,000 stars
  \item $R=22,500$ spectra in $1.5<\lambda<1.7\,\mu\mathrm{m}$
  \item precise RVs and stellar parameters
  \item plan for a dozen abundances for every star
  \item (our own home-built and special continuum normalization; ask me!)
  \item \apogee\ DR12 just happened: 156,000 stars now available
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\sdssiii\ \apogee}
  ~\hfill\includegraphics[height=\figureheight]{../documents/plots/four_examples3.pdf}
\end{frame}

\begin{frame}
  \frametitle{train, validate, and test}
  \begin{itemize}
  \item split the data into three disjoint subsets
  \item in the \emph{training step} you set the parameters of your model using the training set
  \item the validation set is used to set hyperparameters or model complexity
  \item in the \emph{test step} you apply the model to the test set---new data---to make predictions or deliver results
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc: training set}
  \begin{itemize}
  \item 543 stars (too few) from 19 clusters (too few)
  \item $\teff, \logg, \feh$ labels from \apogee
    \begin{itemize}
    \item calling parameters and abundances ``labels''
    \item slight adjustments to labels to get them onto possible isochrones
    \end{itemize}
  \item \emph{terrible} coverage of the main sequence
    \begin{itemize}
    \item only the Pleiades
    \item home-made Pleiades labels (by Ness)
    \item no $\feh$ spread at high $\logg$.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc: training set}
  ~\hfill\includegraphics<1>[height=\figureheight]{../documents/plots/training_aspcap2.pdf}
         \includegraphics<2>[height=\figureheight]{../documents/plots/training_mkn2.pdf}
\end{frame}

\newcommand{\flux}{f}
\newcommand{\fluxes}{\boldsymbol{\flux}}
\newcommand{\labels}{\boldsymbol{\ell}}
\newcommand{\pars}{\boldsymbol{\theta}}

\begin{frame}
  \frametitle{\tc: model}
  \begin{itemize}
  \item a \emph{generative model} of the \apogee\ spectra
    \begin{itemize}
    \item given label vector $\labels$, predict flux vector $\fluxes$
    \item probabilistic prediction $p(\fluxes\given\labels,\pars)$
    \end{itemize}
  \item use every spectral pixel's uncertainty variance $\sigma^2_{\lambda n}$ responsibly
  \item details:
    \begin{itemize}
    \item spectral expectation is quadratic in the labels
    \item every wavelength $\lambda$ treated independently
    \item an intrinsic Gaussian scatter $s^2_\lambda$ at every wavelength $\lambda$
    \item 80,000 free parameters in $\pars$!
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc: model}
  \begin{eqnarray}
    \ln p(\fluxes_n\given\labels_n,\pars) &=& \sum_{\lambda=1}^L \ln p(\flux_{\lambda n}\given\labels_n,\pars_\lambda,s^2_\lambda)
    \nonumber \\
    \ln p(\flux_{\lambda n}\given\labels_n,\pars_\lambda,s^2_\lambda) &=& -\frac{1}{2}\,\frac{[f_{\lambda n} - \transpose{\pars_\lambda}\cdot\labels_n]^2}{\sigma^2_{\lambda n} + s^2_\lambda} + \ln (\sigma^2_{\lambda n} + s^2_\lambda)
    \nonumber \\
    \transpose{\labels} &\equiv& \left\{1, \teff, \logg, \feh, \teff^2, \teff\,\logg, \cdots, \feh^2\right\}
    \nonumber \\
    \transpose{\pars} &\equiv& \left\{\pars_\lambda, s^2_\lambda\right\}_{\lambda=1}^L
    \nonumber
  \end{eqnarray}
  \begin{itemize}
  \item \emph{training step}: optimize parameters $\pars$ at fixed labels
    $\labels$ using training-set data
    \begin{itemize}
    \item linear least squares
    \item every wavelength $\lambda$ treated independently
    \end{itemize}
  \item \emph{test step}: optimize labels $\labels$ at fixed
    parameters $\pars$ using test-set (survey) data
    \begin{itemize}
    \item non-linear optimization
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc: model training}
  ~\hfill\includegraphics<1>[height=\figureheight]{../documents/plots/R1_continuum5.png}
\end{frame}

\begin{frame}
  \frametitle{\tc: model training cross-validation}
  ~\hfill\includegraphics<1>[height=\figureheight]{../documents/plots/takeout_histc.png}
\end{frame}

\newcommand{\results}{%
\begin{frame}
  \frametitle{\tc: results}
  \begin{itemize}
  \item \tc\ is far faster than physical modeling
    \begin{itemize}
    \item model trains in seconds (thousands of least-square fits)
    \item \tc\ labels all 56,000 stars in \apogee\ DR10 in two hours
    \end{itemize}
  \item labels appear sensible
    \begin{itemize}
    \item \tc\ labels lie near sensible isochrones
    \item scatter against \apogee\ labels consistent with \apogee\ precision
    \end{itemize}
  \item successfully puts labels on dwarfs
  \end{itemize}
\end{frame}}

\results

\begin{frame}
  \frametitle{\tc: test time}
  ~\hfill\includegraphics<1>[height=\figureheight]{../documents/plots/4431_v19.png}
         \includegraphics<2>[height=\figureheight]{../documents/plots/4383_v19.png}
         \includegraphics<3>[height=\figureheight]{../documents/plots/4399_v19.png}
         \includegraphics<4>[height=\figureheight]{../documents/plots/4309_v19.png}
         \includegraphics<5>[height=\figureheight]{../documents/plots/4311_v19.png}
         \includegraphics<6>[height=\figureheight]{../documents/plots/4255_v19.png} 
\end{frame}

\begin{frame}
  \frametitle{\tc: comparison with \apogee\ labels}
  ~\hfill\includegraphics[height=\figureheight]{../documents/plots/cplot2.png} 
\end{frame}

\begin{frame}
  \frametitle{\tc: label veracity}
  ~\hfill\includegraphics<1>[height=\figureheight]{../documents/plots/iso2_2.png}
         \includegraphics<2>[height=\figureheight]{../documents/plots/iso2a_2.png}
\end{frame}

\results

\begin{frame}
  \frametitle{\tc: shortcuts and choices}
  \begin{itemize}
  \item no Bayes; no partial or noisy labels
  \item quadratic order
  \item spectral representation
  \item too-small training set
  \item only three labels
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{other applications for data-driven models}
  \begin{itemize}
  \item building a consistent stellar parameter system for \gaia
  \item \kepler\ lightcurves
  \item CMB foregrounds
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Data-driven self-calibration of \kepler\ \small{(Wang \etal)}}
  ~\hfill\includegraphics<1>[height=\figureheight]{./kic_03544595_05_pixels.png}
         \includegraphics<2>[height=\figureheight]{./lightCurve_5088536_1_90_q5_reg1e+05_pdc_outlier.png}
\end{frame}

\begin{frame}
  \frametitle{conservatism requires huge numbers of parameters}
  \begin{itemize}
  \item without enormous freedom, the model won't be data-driven
  \item abandon hope that the number of parameters will be less than the number of data points
  \item marginalization of nuisances
  \end{itemize}
\end{frame}

%\conclusions

\end{document}
